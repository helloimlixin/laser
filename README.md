# LASER: Learnable Adaptive Structured Embedding Representation

This repository provides three autoencoder baselines for image reconstruction:

- **Vector Quantized VAE (VQ-VAE)** â€” discrete latent codes with a learnable codebook.
- **Dictionary Learning VAE (DL-VAE)** â€” sparse dictionary bottleneck trained with Batch OMP.
- **K-SVD VAE (K-SVD-VAE)** â€” dictionary learning with K-SVD updates (classical SVD-based algorithm).

Generation utilities have been removed for now so you can focus on training and evaluating reconstructions only.

## Features

- ðŸš€ Choice of bottlenecks: vector quantization, gradient-based dictionary learning, or K-SVD
- âš¡ GPU-friendly implementation with AMP-aware sparse coding
- ðŸ“Š Reconstruction quality metrics: MSE, PSNR, SSIM, optional LPIPS/FID
- ðŸ”§ Modular architecture powered by PyTorch Lightning and Hydra

## Installation

```bash
# Clone the repository
git clone https://github.com/helloimlixin/laser.git
cd laser

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scriptsctivate

# Install dependencies
pip install -r requirements.txt
```

## Project Structure

```
â”œâ”€â”€ configs/                # Hydra configuration files
â”‚   â”œâ”€â”€ checkpoint/         # Checkpoint configurations
â”‚   â”œâ”€â”€ data/               # Dataset configurations
â”‚   â”œâ”€â”€ model/              # Model configurations
â”‚   â”œâ”€â”€ train/              # Training configurations
â”‚   â”œâ”€â”€ wandb/              # W&B logging configurations
â”‚   â””â”€â”€ config.yaml         # Main configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # Data modules
â”‚   â”‚   â”œâ”€â”€ cifar10.py
â”‚   â”‚   â”œâ”€â”€ imagenette2.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bottleneck.py   # VQ and dictionary bottlenecks
â”‚   â”‚   â”œâ”€â”€ decoder.py
â”‚   â”‚   â”œâ”€â”€ dlvae.py
â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”œâ”€â”€ lpips.py
â”‚   â”‚   â””â”€â”€ vqvae.py
â””â”€â”€ train.py                # Main training script
```

## Usage

### Training

```bash
# Train VQ-VAE
python train.py model=vqvae data=cifar10

# Train DL-VAE
python train.py model=dlvae data=cifar10

# Train LASER
python train.py model=laser data=cifar10
```

### Running Tests

```bash
pytest tests/test_dlvae.py -q
```

## Configuration

All configuration is managed through Hydra. Adjust the YAML files under `configs/` or override settings directly from the command line, e.g.

```bash
python train.py model=dlvae data=celeba train.max_epochs=50
```

## K-SVD Dictionary Learning

### Overview

K-SVD is a classical dictionary learning algorithm that alternates between:
1. **Sparse Coding**: Find sparse coefficients using Orthogonal Matching Pursuit (OMP)
2. **Dictionary Update**: Update each atom sequentially using rank-1 SVD approximations

### Key Differences from Gradient-based DL

| Aspect | K-SVD | Gradient-based DL |
|--------|-------|-------------------|
| Dictionary Update | SVD-based, direct update | Gradient descent with backprop |
| Gradients | No gradients on dictionary | Full gradient computation |
| Convergence | Often faster for small problems | Scales better with large datasets |
| Atom Quality | High-quality, orthogonal atoms | Depends on learning rate & optimization |
| Training Mode | Dictionary updates in forward pass | Standard PyTorch training loop |

### Usage Example

```python
from src.models.bottleneck import DictionaryLearning

# Create Dictionary Learning bottleneck
dl = DictionaryLearning(
    num_embeddings=512,       # Number of dictionary atoms
    embedding_dim=64,         # Dimension of each atom
    sparsity_level=8,         # Number of non-zero coefficients
    commitment_cost=0.25,     # Weight for commitment loss
    ksvd_iterations=1,        # Number of K-SVD updates per forward pass
    patch_size=8,            # Spatial patch size (8x8 patches)
)

# Forward pass
z_reconstructed, loss, coefficients = dl(z_encoded)
```

### K-SVD Parameters

- **num_embeddings**: Number of atoms in the dictionary (codebook size)
- **embedding_dim**: Dimensionality of each atom (typically matches input channels)
- **sparsity_level**: Maximum number of non-zero coefficients per signal
- **commitment_cost**: Weight for the encoder commitment loss term (encourages encoder to match dictionary reconstruction)
- **dictionary_weight**: Weight for the dictionary reconstruction loss (controls how fast dictionary learns via backprop). Default: 1.0
- **ksvd_iterations**: Number of K-SVD dictionary update iterations per forward pass
- **patch_size**: Spatial patch size for patch-based processing (1 = pixel-level, 8 = 8Ã—8 patches)

**Note**: Dictionary atoms are always normalized to unit L2 norm for numerical stability.

### K-SVD Features

1. **Batched OMP Sparse Coding**: Efficient vectorized Orthogonal Matching Pursuit
2. **SVD-based Dictionary Update**: Sequential atom updates using rank-1 approximations
3. **Patch-based Processing**: Reduces computational complexity by processing spatial patches
4. **Automatic Atom Reinitialization**: Prevents dead atoms by reinitializing unused ones
5. **Training/Eval Mode Support**: Dictionary updates enabled in training, frozen in eval

### Dictionary Update Process

For each atom k in the dictionary:
```
1. Find signals using atom k: Ï‰_k = {i : |Î±_k,i| > Îµ}
2. Compute error without atom k: E_k = X - DÂ·Î± + d_kÂ·Î±_k
3. Restrict E_k to signals using k: E_k^Ï‰
4. Perform SVD: E_k^Ï‰ â‰ˆ uÂ·ÏƒÂ·v^T
5. Update atom and coefficients: d_k â† u[:,0], Î±_k[Ï‰_k] â† Ïƒ[0]Â·v[:,0]
```

### Numerical Stability

- SVD computations performed in float32 for stability
- Epsilon values prevent division by zero
- Atom normalization for stable sparse coding
- Graceful handling of SVD failures

### Performance Considerations

**Advantages:**
- No gradient computation overhead for dictionary
- Often converges faster than gradient-based methods
- Produces high-quality, interpretable atoms
- Direct control over atom updates

**Limitations:**
- SVD computation can be expensive for large dictionaries
- Not fully differentiable (uses straight-through estimator)
- Sequential atom updates don't parallelize as well
- Memory usage scales with batch size Ã— num_patches

**Recommendations:**
- Use moderate dictionary sizes (K=256-512) for best performance
- Increase patch_size to reduce number of tokens
- Set ksvd_iterations=1 for faster training
- Dictionary atoms are always normalized for stability

### K-SVD Test Coverage

```bash
# Run K-SVD tests
pytest tests/test_bottleneck.py::test_ksvd_basic -v
pytest tests/test_bottleneck.py::test_ksvd_training -v
pytest tests/test_bottleneck.py::test_ksvd_patches -v
```

### LASER Architecture

```python
from src.models.laser import LASER

# Initialize LASER
model = LASER(
    in_channels=3,
    num_hiddens=128,
    num_embeddings=512,
    embedding_dim=64,
    sparsity_level=8,
    num_residual_blocks=2,
    num_residual_hiddens=64,
    commitment_cost=0.25,
    learning_rate=1e-4,
    ksvd_iterations=1,
    patch_size=8,  # 8x8 spatial patches
)

# Forward pass
x_recon, loss, coeffs = model(x)
```

The straight-through estimator ensures gradients flow back to the encoder during training, while the dictionary can be updated either:
1. **Via K-SVD/Online Learning**: Direct updates using SVD or gradient-like online learning (no gradients)
2. **Via Backpropagation**: Gradient-based learning controlled by `dictionary_weight` parameter

### Dictionary Learning Methods

K-SVD VAE supports three dictionary learning modes and multiple sparse coding algorithms:

### Sparse Coding Algorithms

The bottleneck supports three sparse coding methods via the `sparse_solver` parameter:

#### 1. OMP (Orthogonal Matching Pursuit) - Default
```yaml
sparse_solver: omp
```

**Description**: Greedy algorithm with least-squares refinement at each iteration.

**Advantages:**
- âœ“ Best reconstruction quality
- âœ“ Proper orthogonalization of selected atoms
- âœ“ Least-squares coefficient refinement

**Disadvantages:**
- âœ— Slower than top-k (requires solving linear system at each iteration)
- âœ— More complex implementation

**When to Use:**
- When reconstruction quality is critical
- Small to medium datasets
- When you can afford the computational cost

#### 2. IHT (Iterative Hard Thresholding) - Recommended for Speed/Quality Balance
```yaml
sparse_solver: iht
iht_iterations: 10      # More iterations = better approximation
iht_step_size: null     # Auto-compute from spectral norm (recommended)
```

**Description**: True iterative sparse coding that alternates between gradient steps and hard thresholding.

**Algorithm:**
```python
for iteration in range(iht_iterations):
    residual = X - D @ coefficients
    gradient = D.T @ residual
    coefficients += step_size * gradient
    coefficients = hard_threshold(coefficients, k=sparsity_level)
```

**Advantages:**
- âœ“ True sparse coding (proper optimization algorithm)
- âœ“ Fast (simple matrix-vector operations)
- âœ“ Theoretically grounded (converges under RIP condition)
- âœ“ Adjustable quality via `iht_iterations`
- âœ“ Automatic step size from spectral norm

**Disadvantages:**
- âœ— Slower than top-k (but faster than OMP)
- âœ— Requires multiple iterations to converge

**When to Use:**
- **Recommended default** for most use cases
- Good balance between speed and quality
- When you want proper sparse coding without OMP overhead
- Training on medium to large datasets

**Tuning:**
- `iht_iterations=5-10`: Good balance (default: 10)
- `iht_iterations=15-20`: Better quality, slower
- `iht_step_size=null`: Auto-compute (recommended)
- `iht_step_size=0.5-1.0`: Manual override if needed

#### 3. Top-K - Fastest (Not Recommended)
```yaml
sparse_solver: topk
```

**Description**: Simple selection of top-k atoms by correlation (single matrix multiplication).

**Advantages:**
- âœ“ Fastest (single matmul + top-k)
- âœ“ Simplest implementation

**Disadvantages:**
- âœ— Not true sparse coding (just selection, no optimization)
- âœ— Can select weak atoms, leading to overfitting
- âœ— No iterative refinement

**When to Use:**
- Only when speed is absolutely critical
- Prototyping / quick experiments
- Not recommended for final models

### Dictionary Learning Methods

#### 1. Gradient-Based Learning (Default - Fastest)
```yaml
# configs/model/laser.yaml
use_online_learning: false
ksvd_iterations: 0
dictionary_weight: 5.0  # Higher = faster dictionary updates
```

**Loss Formula:**
```python
loss = commitment_cost * e_latent_loss + dictionary_weight * dl_latent_loss
loss = 0.25 * encoder_loss + 5.0 * dictionary_loss
```

Where:
- `e_latent_loss` (encoder commitment): MSE(encoder_output, dictionary_reconstruction.detach()) - gradients flow to encoder
- `dl_latent_loss` (dictionary reconstruction): MSE(dictionary_reconstruction, encoder_output.detach()) - gradients flow to dictionary

**Key Parameters:**
- `dictionary_weight`: Controls dictionary learning speed (default: 1.0, recommended: 2.0-10.0)
  - Higher values â†’ stronger gradients â†’ faster dictionary adaptation
  - Lower values â†’ slower, more stable dictionary learning
  - Similar to VQ-VAE's codebook loss weight (typically 1.0)

**Advantages:**
- âœ“ Fastest training (no SVD computation)
- âœ“ Full integration with PyTorch autograd
- âœ“ Scales well with large dictionaries
- âœ“ Easy to tune via `dictionary_weight`

**When to Use:**
- Large-scale datasets (ImageNet, etc.)
- Large dictionaries (K > 512)
- When speed is critical
- When you want standard gradient-based optimization

#### 2. Online Dictionary Learning
```yaml
use_online_learning: true
ksvd_iterations: 0
dict_learning_rate: 0.5  # Higher = faster updates
```

Fast vectorized updates similar to online SGD for dictionaries:
```python
gradient = residual @ coefficients.T
dictionary += dict_learning_rate * gradient / usage_counts
```

**Advantages:**
- âœ“ Fast (vectorized, no SVD)
- âœ“ Direct dictionary control
- âœ“ No gradient computation overhead

**When to Use:**
- Medium-scale datasets
- When you want control over dictionary updates separate from encoder
- Moderate dictionary sizes (K=256-512)

#### 3. K-SVD Updates (Classical)
```yaml
use_online_learning: false
ksvd_iterations: 2
```

Classical K-SVD with SVD-based atom refinement.

**Advantages:**
- âœ“ High-quality atoms
- âœ“ Often faster convergence on small datasets

**Disadvantages:**
- âœ— Slower (SVD computation)
- âœ— Sequential atom updates

**When to Use:**
- Small datasets (CIFAR-10, MNIST)
- When interpretability is critical
- Research/analysis of dictionary structure

### Tuning Dictionary Learning Speed

For gradient-based learning, control update speed via `dictionary_weight`:

```bash
# Slow, stable dictionary learning
python train.py model=laser model.dictionary_weight=1.0

# Moderate speed (recommended starting point)
python train.py model=laser model.dictionary_weight=5.0

# Fast dictionary adaptation
python train.py model=laser model.dictionary_weight=10.0
```

**Effect on Training:**
- Higher `dictionary_weight` â†’ Dictionary adapts faster to encoder outputs
- Lower `dictionary_weight` â†’ More stable but slower dictionary learning
- Balance with `commitment_cost` (encoder side) for best results

**Comparison with VQ-VAE:**
```python
# VQ-VAE loss (for reference)
loss = q_latent_loss + commitment_cost * e_latent_loss
loss = 1.0 * codebook_loss + 0.25 * encoder_loss

# K-SVD VAE loss (gradient-based)
loss = commitment_cost * e_latent_loss + dictionary_weight * dl_latent_loss
loss = 0.25 * encoder_loss + 5.0 * dictionary_loss
```

With `dictionary_weight=5.0`, the dictionary receives **5Ã— stronger learning signal** than VQ-VAE's codebook!

### K-SVD Training Guide

#### Training Commands

Train K-SVD VAE on different datasets:

```bash
# CIFAR-10 (32Ã—32 images)
python train.py model=laser data=cifar10

# CelebA (256Ã—256 images)
python train.py model=laser data=celeba

# ImageNette2 (256Ã—256 images)
python train.py model=laser data=imagenette2
```

Override specific parameters:

```bash
python train.py model=laser data=celeba \
    model.num_embeddings=256 \
    model.sparsity_level=16 \
    model.ksvd_iterations=2 \
    model.patch_size=4
```

#### Key Differences from DLVAE

| Aspect | Regular DLVAE | LASER |
|--------|---------------|-----------|
| Dictionary Update | Gradient descent | SVD-based direct update |
| Convergence | Slower, needs many epochs | Faster, updates in forward pass |
| Gradients | Full backprop through dictionary | No gradients on dictionary |
| Training Mode | Dictionary learned via gradients | Dictionary updated via K-SVD |
| Best For | Large-scale datasets | Smaller datasets, interpretability |

#### Logged Metrics

The following metrics are tracked during training:

- **Loss metrics**: `train/loss`, `val/loss` (total loss)
- **Component losses**: 
  - `train/recon_loss`, `val/recon_loss` (MSE reconstruction)
  - `train/ksvd_loss`, `val/ksvd_loss` (K-SVD bottleneck loss)
  - `train/perceptual_loss`, `val/perceptual_loss` (LPIPS)
  - `train/mr_dct_loss`, `val/mr_dct_loss` (multi-resolution DCT)
  - `train/mr_grad_loss`, `val/mr_grad_loss` (multi-resolution gradient)
- **Quality metrics**: 
  - `train/psnr`, `val/psnr` (Peak Signal-to-Noise Ratio)
  - `train/ssim`, `val/ssim` (Structural Similarity Index)
- **Sparsity**: `train/sparsity`, `val/sparsity` (average number of active atoms)

#### Tips for Best Results

1. **Start Small**: Begin with fewer atoms (256-512) and moderate sparsity (8-16)

2. **Tune K-SVD Iterations**: 
   - 1-2 iterations for speed
   - 3-5 iterations for quality

3. **Patch Size**:
   - Use 1 (pixel-level) for best quality but slower training
   - Use 4-8 for faster training on large images with some quality trade-off

4. **Perceptual Loss**: Essential for good visual quality
   - Set to 0.5-1.0 for best results

5. **Monitor Sparsity**: Should stay around `sparsity_level / num_embeddings`
   - If too high: increase commitment_cost
   - If too low: decrease commitment_cost

#### Troubleshooting

**Dictionary atoms not being used**
- Solution: Increase `ksvd_iterations` or decrease `sparsity_level`

**Training is slow**
- Solutions:
  - Reduce `ksvd_iterations` to 1
  - Increase `patch_size` to 4 or 8
  - Reduce `num_embeddings`

**Poor reconstruction quality**
- Solutions:
  - Increase `sparsity_level`
  - Increase `num_embeddings`
  - Increase `perceptual_weight`
  - Decrease `patch_size` to 1 (pixel-level)

**Validation loss not improving (overfitting)**
- Solutions:
  - Reduce dictionary update frequency
  - Increase encoder/decoder capacity
  - Use data augmentation
  - Lower `patch_size` for finer granularity

**SVD errors during training**
- Solution: This is handled internally with try-except. If it persists, reduce learning rate to stabilize encoder outputs.

#### Evaluation

After training, load and evaluate checkpoints:

```python
from src.models.laser import LASER

# Load checkpoint
model = LASER.load_from_checkpoint('outputs/checkpoints/run_xxx/last.ckpt')
model.eval()

# Run inference
with torch.no_grad():
    recon, loss, coeffs = model(images)
```

### Additional References

- Y. C. Pati, R. Rezaiifar and P. S. Krishnaprasad, "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition," Asilomar Conference on Signals, Systems and Computers, 1993.

### References

M. Aharon, M. Elad and A. Bruckstein, "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation," in IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311-4322, Nov. 2006.

## Bottleneck Visualizations

The `tests/test_bottleneck.py` generates comprehensive visualizations comparing Vector Quantization (VQ) and Dictionary Learning (DL) bottlenecks on CelebA data at 128Ã—128 resolution.

### Reconstruction Comparison

Side-by-side comparison of VQ and DL reconstruction quality (K=16 codebook/atoms, S=4 sparsity):

![Reconstruction Comparison](img/reconstruction_comparison.png)

**Key Observations:**
- **VQ (Vector Quantization)**: Maps each pixel to the nearest RGB color from 16 learned codebook entries. Fast but limited to discrete color matching.
- **DL (Dictionary Learning)**: Represents each pixel as a sparse combination of 4 atoms from 16 options. More expressive representation.
- **Error Maps**: DL shows consistently lower error (darker error maps) across all images, especially in complex regions like faces.
- **Quantitative**: DL achieves **10.4Ã— lower MSE** than VQ while using the same 16-entry codebook, demonstrating the power of sparse combinations.

### Code Interpretability Heatmaps

Spatial visualization of how VQ and DL encode the image structure:

![Code Heatmaps](img/code_heatmaps.png)

**Understanding the Heatmaps:**
- **Column 1 (Original)**: Input CelebA images at 128Ã—128 resolution
- **Column 2 (VQ Code Indices)**: Shows which of the 16 codebook entries is assigned to each pixel. Color represents the discrete code index (0-15).
  - Each pixel uses exactly **1 code** from 16 options
  - Spatial patterns show how VQ segments the image into color regions
  - Viridis colormap: Purple (low indices) â†’ Yellow (high indices)
- **Column 3 (DL Sparse Code Strength)**: Shows the L1 norm (sum of absolute coefficients) at each pixel location.
  - Each pixel uses **4 different atoms** with varying weights
  - Darker regions (purple) = weak total activation, brighter regions (yellow) = strong total activation
  - Reveals which image areas require stronger sparse representations
  - L1 norm provides a stable measure of total "activation energy" per location
  - Normalized to [0, 1] using percentile clipping (1st-99th percentile) for clean visualization

**VQ vs DL Encoding:**
- VQ: Discrete, categorical assignment (one-hot selection)
- DL: Continuous, weighted combination (sparse weighted sum)
- DL's flexibility with L1-normalized coefficients enables better reconstruction with the same codebook size

### Channel-wise Comparison

Pixel-level RGB channel analysis comparing original vs reconstructions:

![Channel Comparison](img/vq_channel_comparison.png)

**Analysis:**
- Shows a horizontal slice through the center of the first image across all three color channels
- **Black line**: Original pixel values
- **Blue dashed**: VQ reconstruction
- **Red dotted**: DL reconstruction
- **Key Insight**: DL tracks the original signal more closely than VQ across all channels, especially for smooth gradients and transitions
- Both VQ and DL handle sharp edges well, but DL excels at subtle color variations

### Usage Statistics

How frequently each codebook entry (VQ) or dictionary atom (DL) is utilized:

**VQ Codebook Usage:**
![VQ Usage](img/vq_codebook_usage.png)

- **All 16 codes are used**, showing k-means initialization creates a representative color palette
- Usage distribution is moderately skewed (dominant colors like skin, hair, background are used more)

**Dictionary Atom Usage:**
![DL Usage](img/dictionary_atom_usage.png)

- **All 16 atoms are used** thanks to diversity bonus in OMP selection
- More balanced distribution than VQ due to sparse combination (each pixel can use multiple atoms)
- Top atoms represent dominant color directions, while less-used atoms capture nuanced variations

### Detailed Performance Analysis (K=16, Sparsity=4, 128Ã—128 resolution)

#### Overall Reconstruction Quality

| Method | Overall MSE | Per-Pixel Color Distance | Quality Gain |
|--------|-------------|-------------------------|--------------|
| **VQ** | 0.00905 | 0.0759 | Baseline |
| **DL** | 0.00087 | 0.0163 | **10.4Ã— better MSE**, **21% of VQ color error** |

**Key Insight**: With the simplified greedy OMP, DL reduces per-pixel color distance to just 21% of VQ's error, demonstrating dramatically better perceptual quality. The greedy algorithm provides excellent reconstruction while being simpler and faster than Cholesky-based OMP.

#### Channel-wise Performance

Breaking down reconstruction quality by RGB channel reveals DL's exceptional improvement:

| Channel | VQ MSE | DL MSE | DL Improvement |
|---------|--------|--------|----------------|
| **Red** | 0.0103 | 0.0003 | **30.8Ã— better** |
| **Green** | 0.0084 | 0.0013 | **6.3Ã— better** |
| **Blue** | 0.0085 | 0.0010 | **8.9Ã— better** |

**Analysis**: 
- DL excels particularly in Red channel (30Ã—+ improvement!)
- All channels show significant improvement (6-31Ã— better)
- Green channel shows most modest but still strong improvement (6.3Ã—)
- Balanced performance across all channels indicates proper sparse coding without artifacts

#### Codebook/Atom Utilization

| Method | Entries Used | Distribution | Notes |
|--------|--------------|--------------|-------|
| **VQ** | 16/16 (100%) | Moderately skewed | Each pixel uses exactly 1 code |
| **DL** | 16/16 (100%) | Balanced | Each pixel uses exactly 4 different atoms |

**Full Utilization**: Both methods use all 16 codebook entries/atoms thanks to k-means initialization (VQ) and greedy selection with no-reselection masking (DL), ensuring the full representation capacity is utilized.

#### Inference Speed & Computational Complexity

**Benchmark Setup**: 128Ã—128 resolution, K=16 atoms, S=4 sparsity

| Batch Size | Pixels | VQ Time | VQ Âµs/pixel | DL Time | DL Âµs/pixel | Slowdown |
|------------|--------|---------|-------------|---------|-------------|----------|
| **1** | 16,384 | 1.2 ms | 0.070 | 9.0 ms | 0.551 | **7.9Ã—** |
| **4** | 65,536 | 1.8 ms | 0.028 | 33.0 ms | 0.503 | **17.9Ã—** |
| **8** | 131,072 | 2.6 ms | 0.020 | 64.5 ms | 0.492 | **24.5Ã—** |
| **16** | 262,144 | 5.0 ms | 0.019 | 128.6 ms | 0.490 | **26.0Ã—** |

**Complexity Analysis**:
- **VQ**: O(K Ã— M Ã— N) where K=codebook size, M=channels, N=num_pixels
  - Perfectly vectorizable across batch and spatial dimensions
  - Excellent batching efficiency: per-pixel time drops 3.7Ã— (70ns â†’ 19ns) as batch grows
  - Single matrix multiplication D^T @ X for all pixels simultaneously
  
- **DL**: O(S Ã— K Ã— M Ã— N) where S=sparsity, K=atoms, M=channels, N=num_pixels  
  - **Sequential within each iteration**: correlation â†’ argmax â†’ update â†’ repeat S times
  - Limited batching benefit: per-pixel time only improves 1.1Ã— (551ns â†’ 490ns)
  - Each of S=4 iterations requires KÃ—M matrix-vector products per pixel
  
**Why DL Scales Worse:**
1. **Sequential dependencies**: Each OMP iteration depends on previous iteration's residual
2. **Less vectorization**: VQ does one big matmul; OMP does 4 sequential smaller operations
3. **Memory access**: OMP repeatedly reads/writes residuals; VQ has cleaner access pattern
4. **Fixed overhead**: OMP initialization costs are amortized less efficiently

**Theoretical vs Measured Slowdown:**
- **Theoretical**: 4Ã— (from S=4 sparsity iterations)
- **Measured**: 7.9-26Ã— depending on batch size
- **Gap explained by**: Sequential iteration overhead, less efficient vectorization, memory access patterns

**Scaling Observations**:
- **VQ scales excellently with batch size**: Per-pixel cost drops 3.7Ã— as vectorization efficiency improves
- **DL scaling is limited by sequential OMP iterations**: Each iteration must complete before next begins
- **Relative slowdown grows with batch size**: From 7.9Ã— (batch=1) to 26Ã— (batch=16)
  - This is expected: VQ's better vectorization means it benefits more from larger batches
  - DL's per-pixel time stays nearly constant (~0.50 Âµs) regardless of batch size
- **Both methods remain practical**: VQ at 19-70 ns/pixel, DL at 490-551 ns/pixel
- **Quality advantage dominates**: Despite 8-26Ã— slowdown, DL achieves **10.4Ã— better MSE**

**Speed vs Quality Tradeoff**:
- At batch=4 (typical training batch): DL is 17.9Ã— slower but achieves **10.4Ã— better MSE**
- For batch processing and offline training, the quality gain far outweighs the speed cost
- Both methods support real-time processing: even at batch=16, DL processes 262K pixels in 129ms

#### Patch-Based Dictionary Learning

Using larger patches dramatically reduces computation by processing fewer tokens, with a tradeoff in reconstruction quality:

| Patch Size | Patches/Image | DL Time | Speedup | DL MSE | VQ MSE | Quality vs VQ |
|------------|---------------|---------|---------|--------|--------|---------------|
| **1Ã—1** (pixel) | 65,536 | 33.6 ms | 1.0Ã— | 0.00817 | 0.00964 | **DL 1.2Ã— better** âœ“ |
| **2Ã—2** | 16,384 | 9.3 ms | **3.6Ã—** | 0.01071 | 0.00964 | VQ 1.1Ã— better â‰ˆ |
| **4Ã—4** | 4,096 | 3.7 ms | **9.1Ã—** | 0.06227 | 0.00964 | VQ 6.5Ã— better âœ— |
| **8Ã—8** | 1,024 | 2.3 ms | **14.8Ã—** | 0.09623 | 0.00964 | VQ 10Ã— better âœ— |

**Key Findings**:
- **Pixel-level DL best quality**: 1.2Ã— better than VQ with full spatial resolution
- **2Ã—2 patches competitive**: Only 1.1Ã— worse than VQ but 3.6Ã— faster than pixel-level DL
- **Larger patches trade quality for speed**: 4Ã—4 and 8Ã—8 achieve 9-15Ã— speedup but sacrifice quality
- **Proper initialization critical**: Dictionary must be initialized with k-means on patches (not pixels!)

**Why Larger Patches Degrade**:
1. Higher-dimensional atoms (e.g., 8Ã—8 RGB patch = 192D) harder to represent with fixed sparsity
2. Each patch is single token â†’ coarser spatial granularity than pixel-level
3. K-means initialization on high-D patch space less effective than on pixels
4. Fixed 4-atom sparsity insufficient for complex patch patterns

**Practical Recommendations**:
- **High quality needed**: Use **1Ã—1 (pixel-level)** for 1.2Ã— better MSE than VQ
- **Speed/quality balance**: Use **2Ã—2 patches** for 3.6Ã— speedup with competitive quality (1.1Ã— worse)
- **Maximum speed**: Use **4Ã—4 patches** for 9Ã— speedup if some quality loss acceptable
- **Avoid 8Ã—8**: Too coarse, loses too much detail (10Ã— worse than VQ)

**Conclusion**: **Patch-based DL offers a flexible speed/quality tradeoff**. For most use cases, **2Ã—2 patches** provide the best balance: 3.6Ã— faster than pixel-level with only marginal quality loss. Pixel-level remains best for maximum quality.

### Key Advantages of Dictionary Learning

- âœ“ **Superior reconstruction**: 10.4Ã— lower MSE with same 16-entry codebook
- âœ“ **Sparse representation**: Each pixel uses only 4/16 atoms (25% sparsity) vs VQ's 1/16 selection
- âœ“ **Perceptual quality**: 21% color distance of VQ (nearly 5Ã— improvement)
- âœ“ **Channel balance**: Outperforms VQ on all RGB channels (6-31Ã— better)
- âœ“ **Full utilization**: All 16 atoms actively contribute to reconstruction
- âœ“ **Interpretable**: L1 norm visualization reveals spatial importance patterns (see heatmaps)
- âœ“ **Gradient-friendly**: Supports end-to-end training with proper backpropagation
- âœ“ **Simpler implementation**: Greedy OMP is cleaner and faster than Cholesky-based approaches

### Tradeoffs

- âœ— **Slower inference**: 7.9-26Ã— slower than VQ depending on batch size (larger batches increase relative slowdown)
- âœ— **Memory overhead**: Must store and compute with full dictionary + sparse coefficients  
- âœ— **Computational complexity**: O(K Ã— S Ã— N) vs VQ's O(K Ã— N) where K=atoms, S=sparsity, N=pixels
- âœ— **Worse batching scaling**: VQ benefits more from large batches due to simpler vectorization

**Bottom Line**: Despite being 8-26Ã— slower (depending on batch size), DL achieves **10.4Ã— better reconstruction quality**. Both methods remain practical for real-time use with sub-microsecond per-pixel processing. For applications where quality matters, DL's superior reconstruction far outweighs the speed cost.

### Codebook Visualization in RGB Space

To understand how VQ and DL learn different representations, we can visualize their codebooks/dictionaries in 2D using dimensionality reduction:

![Codebook Embeddings](img/codebook_embeddings.png)

**Key Observations**:
- **PCA (Linear)**: Both VQ and DL spread their atoms across similar RGB subspaces, with the first two principal components capturing ~85% variance
- **t-SNE (Non-linear)**: Reveals local clustering structure - both methods form distinct color clusters (e.g., skin tones, hair colors, backgrounds)
- **UMAP (Manifold)**: Shows the global topology - atoms are distributed along a smooth manifold representing the continuous RGB color space

**Pairwise Distance Analysis**:

![Codebook Distances](img/codebook_distances.png)

- **VQ**: Mean pairwise distance = 2.49 Â± 0.90 (more uniform spacing)
- **DL**: Mean pairwise distance = 2.48 Â± 0.90 (similar distribution)

Both methods achieve similar codebook diversity, but DL's advantage comes from **sparse combinations** (4 atoms per pixel) rather than just better atom selection.

Run codebook visualization:
```bash
conda activate research
pytest tests/test_codebook_visualization.py::test_visualize_codebook_embeddings -v
```

### Technical Implementation

**Simplified Greedy OMP**:
- ðŸš€ **Greedy atom selection**: Simplified OMP without Cholesky decomposition (faster and cleaner)
- ðŸš€ **Vectorized batch processing**: (N, B) tensor format for efficient parallel sparse coding
- ðŸš€ **Float masking**: Uses multiplicative masking instead of boolean indexing for speed
- ðŸš€ **Projection-based coefficients**: Direct inner product computation (no least squares solve)
- ðŸš€ **No-reselection masking**: Ensures each signal uses exactly S distinct atoms

**Visualization Improvements**:
- ðŸ“Š **L1 norm visualization**: Uses sum of absolute coefficients for stable, interpretable heatmaps
- ðŸ“Š **Percentile normalization**: Robust 1-99th percentile clipping for clean contrast
- ðŸ“Š **Fold/unfold mapping**: Properly maps patch-based coefficients to pixel space for visualization
- ðŸ“Š **RGB space embeddings**: PCA, t-SNE, and UMAP projections reveal codebook structure and diversity

Run visualizations:
```bash
conda activate research
pytest tests/test_bottleneck.py::test_bottleneck_visualizations -v

# Update README images after regenerating visualizations
cp tests/artifacts/bottleneck/*.png img/
cp tests/artifacts/codebook_embeddings/*.png img/
```

## Generation Capabilities

### VQ-VAE for Generation âœ… (RECOMMENDED)

VQ-VAE is **ideal for generation** because it produces discrete codes:

```
Image â†’ Encoder â†’ VQ (discrete codes) â†’ Decoder â†’ Image
                      â†“
                [42, 17, 8, 103, ...]  # Discrete sequence
                      â†“
           Transformer/Diffusion Model âœ…
```

**Advantages:**
- âœ… Clean discrete token sequence (like text)
- âœ… Standard transformer architecture works out-of-the-box
- âœ… Easy to train autoregressive/diffusion models
- âœ… Well-established (DALL-E, Parti, Imagen use VQ-style tokenization)
- âœ… Can use any sequence model (GPT, BERT, diffusion)

**Example:**
```python
# Encode images to discrete codes
indices, H, W = vqvae.encode_to_indices(images)  # [B, H*W]

# Train transformer on code sequences
transformer = GPT(vocab_size=512, ...)
transformer.fit(indices)

# Generate new images
new_codes = transformer.generate(temperature=1.0)
new_images = vqvae.decode_from_indices(new_codes, H, W)
```

### Dictionary Learning for Generation âŒ (NOT RECOMMENDED)

Dictionary learning produces **continuous sparse coefficients**, which are hard to model:

```
Image â†’ Encoder â†’ Sparse Coding â†’ Dictionary â†’ Decoder
                       â†“
          [0.3, 0, -0.2, 0, ..., 0.5]  # Continuous sparse vector
                       â†“
           ??? How to model this ???
```

**Challenges:**
- âŒ Continuous sparse vectors (not discrete tokens)
- âŒ Hard to model with transformers (need regression, not classification)
- âŒ No clear "token" concept for sequence modeling
- âŒ Sparsity constraint complicates generation
- âŒ Each patch has K continuous values (e.g., 8 coefficients)

**Why it's harder:**
```python
# VQ-VAE: Predict 1 discrete code per location
logits = transformer(previous_codes)  # Shape: [B, seq_len, vocab_size]
next_code = logits.argmax(dim=-1)  # Easy!

# Dictionary Learning: Predict K continuous coefficients per location
coeffs = model(previous_coeffs)  # Shape: [B, seq_len, K]
# Need to:
# 1. Predict K values (regression)
# 2. Maintain sparsity (only K non-zero)
# 3. Select which atoms to use (combinatorial problem)
# Much harder!
```

### What Dictionary Learning IS Good For

Dictionary Learning excels at:
- âœ… **Reconstruction quality**: Superior MSE/PSNR for same codebook size
- âœ… **Compression**: Better rate-distortion tradeoff
- âœ… **Feature extraction**: Interpretable sparse features
- âœ… **Analysis**: Understanding data structure
- âœ… **Denoising/Inpainting**: Non-generative tasks

### Hybrid Approach: Dictionary + VQ (FUTURE WORK)

For best of both worlds, you could add VQ on top of sparse codes:

```
Image â†’ Encoder â†’ Sparse Coding â†’ VQ Layer â†’ Discrete Codes â†’ Decoder
                       â†“              â†“
                Sparse features   Discrete tokens
                (Better quality)  (Easy generation)
```

This would give:
1. Dictionary learning benefits (better reconstruction)
2. Discrete codes for easy generation
3. Standard transformer training

**Implementation sketch:**
```python
class HybridVAE(pl.LightningModule):
    def __init__(self):
        self.encoder = Encoder(...)
        self.sparse_coding = DictionaryLearning(...)
        self.vq_on_codes = VectorQuantizer(...)  # VQ the sparse codes
        self.decoder = Decoder(...)

    def forward(self, x):
        z = self.encoder(x)
        sparse_coeffs = self.sparse_coding(z)  # Continuous sparse
        discrete_codes = self.vq_on_codes(sparse_coeffs)  # Discrete!
        return self.decoder(discrete_codes)
```

### Recommendation

**For generation tasks:**
- Use **VQ-VAE** (discrete codes â†’ easy transformer training)
- Train autoregressive/diffusion model on codes
- Standard, proven approach

**For reconstruction/compression only:**
- Use **Dictionary Learning** (better quality, but no generation)
- Superior reconstruction metrics
- Good for analysis and understanding

## Sparse Coding Performance Comparison

### Algorithm Comparison Summary

| Algorithm | Speed | Quality | Sparsity Enforcement | Overfitting Risk | Recommendation |
|-----------|-------|---------|---------------------|------------------|----------------|
| **OMP** | Slow | Best | Strong (LS refinement) | Low | Use for best quality |
| **IHT** | Medium | Good | Strong (iterative) | Low | **Recommended default** |
| **Top-K** | Fast | Poor | Weak (just selection) | High | Avoid for production |

### Why IHT (Iterative Hard Thresholding) is Recommended

**IHT Algorithm:**
```python
# Initialize coefficients to zero
Î± = 0

# Iterate to minimize ||X - D*Î±||Â² subject to ||Î±||â‚€ â‰¤ k
for t in range(iterations):
    residual = X - D @ Î±           # Compute reconstruction error
    gradient = D.T @ residual      # Gradient of loss
    Î± = Î± + step_size * gradient   # Gradient descent step
    Î± = HardThreshold_k(Î±)         # Keep only top-k coefficients
```

**Key Advantages:**
1. âœ… **True Sparse Coding**: Proper optimization algorithm (not just selection)
2. âœ… **Iterative Refinement**: Progressively minimizes reconstruction error
3. âœ… **Theoretical Guarantees**: Converges to local optimum under RIP condition
4. âœ… **Prevents Overfitting**: Hard thresholding enforces sparsity at each iteration
5. âœ… **Faster than OMP**: Simple matrix ops, no linear system solve
6. âœ… **Better than Top-K**: Optimizes reconstruction, not just correlation

**Performance Metrics (expected with IHT):**
- `train/sparsity`: ~0.03-0.10 (true sparsity!)
- Lower `train/recon_loss` than top-k
- Smaller train/val gap (better generalization)
- Stable training dynamics

### Top-K Problems (Why We Switched Away)

**Top-K is NOT sparse coding:**
```python
# Top-K: Just selects atoms with highest correlation
correlations = D.T @ X
top_k_indices = argmax_k(abs(correlations))
coefficients[top_k_indices] = correlations[top_k_indices]
# Problem: No optimization! Just correlation-based selection
```

**Issues with Top-K:**
1. âŒ **Not an optimization algorithm** - just greedy selection
2. âŒ **Selects weak atoms** - can pick atoms with low correlation
3. âŒ **No iterative refinement** - single pass, no improvement
4. âŒ **Overfitting** - observed sparsity ~1.0 (uses almost all atoms!)
5. âŒ **No theoretical guarantees** - not guaranteed to minimize reconstruction error

**Observed Problems in Training:**
```
With Top-K:
  train/sparsity: 0.9999 (99.99% - essentially dense!)
  train/recon_loss: higher
  val/loss >> train/loss (overfitting)

With IHT:
  train/sparsity: 0.03-0.10 (3-10% - truly sparse!)
  train/recon_loss: lower (proper optimization)
  val/loss â‰ˆ train/loss (better generalization)
```

### IHT Configuration & Tuning

**Default Configuration (Recommended):**
```yaml
# configs/model/laser.yaml
sparse_solver: iht
iht_iterations: 10          # Good balance
iht_step_size: null         # Auto-compute (recommended)
sparsity_level: 8           # Number of non-zero coefficients
sparsity_reg_weight: 0.01   # L1 regularization
```

**Tuning Guidelines:**

**For Better Quality (slower training):**
```yaml
iht_iterations: 15-20       # More iterations
iht_step_size: null         # Keep auto
```

**For Faster Training (slight quality loss):**
```yaml
iht_iterations: 5-7         # Fewer iterations
iht_step_size: 0.9          # Manually set (skip spectral norm computation)
```

**For Maximum Sparsity:**
```yaml
sparsity_level: 4-6         # Fewer active atoms
sparsity_reg_weight: 0.05   # Stronger L1 penalty
iht_iterations: 15          # More iterations for better approximation
```

### Step Size Selection

IHT requires step size Î¼ â‰¤ 1/L where L = ||D||Â²â‚‚ (spectral norm squared).

**Auto-compute (default):**
```yaml
iht_step_size: null
```
- Uses fast power iteration to estimate spectral norm
- Conservative step size: Î¼ = 0.9 / LÂ²
- Recommended for most use cases

**Manual override:**
```yaml
iht_step_size: 0.5-1.0
```
- Faster (skips spectral norm computation)
- Use if dictionary is well-normalized (||D||â‚‚ â‰ˆ 1)
- Start with 0.9, reduce if training unstable

### Monitoring IHT Training

**Healthy Training Indicators:**
```
âœ… train/sparsity: 0.03-0.10
âœ… train/sparsity_loss: decreasing
âœ… train/recon_loss: steadily decreasing  
âœ… val/loss â‰ˆ train/loss (gap < 20%)
âœ… val/psnr: increasing over epochs
```

**Warning Signs:**
```
âš ï¸ train/sparsity > 0.5 (too many atoms used)
   â†’ Increase sparsity_reg_weight
   
âš ï¸ train/recon_loss not decreasing
   â†’ Reduce iht_step_size or increase iht_iterations
   
âš ï¸ val/loss >> train/loss (overfitting)
   â†’ Increase sparsity_reg_weight and weight_decay
```

### When to Use Each Sparse Solver

| Use Case | Recommended Solver | Config |
|----------|-------------------|--------|
| **Production training** | IHT | `iht_iterations=10` |
| **Best quality** | OMP | `sparse_solver=omp` |
| **Fast prototyping** | IHT | `iht_iterations=5` |
| **Research/analysis** | OMP | For interpretability |
| **Maximum speed** | Top-K | Only if quality not critical |

### Implementation Details

**IHT Improvements in This Repo:**
1. âœ… **Fast spectral norm estimation** - Power iteration (3 iterations)
2. âœ… **Conservative step size** - Î¼ = 0.9/LÂ² for stability
3. âœ… **Efficient hard thresholding** - Vectorized top-k operation
4. âœ… **Automatic fallback** - Assumes ||D||â‚‚ â‰ˆ 1 if computation fails
5. âœ… **Proper gradient flow** - Compatible with straight-through estimator

**Code Reference:**
```python
# See src/models/bottleneck.py::iterative_hard_thresholding()
# Fully documented implementation with theoretical background
```

## Overfitting Prevention & Regularization

### Common Overfitting Issues

During training, LASER models can exhibit overfitting characterized by:
- Large gap between training and validation loss
- Near-perfect sparsity (1.0) indicating dictionary memorization
- Excellent training metrics but poor generalization

### Issues Identified & Fixes

#### 1. **Dictionary Memorization (Critical)**

**Problem**: Training sparsity metric shows ~1.0 (100%), meaning sparse coding selects nearly all atoms instead of a sparse subset. This defeats sparsity and allows memorization.

**Evidence**:
- `train/sparsity` consistently at 0.9999+ (essentially 1.0)
- Each sample uses almost all dictionary atoms (not sparse!)

**Root Cause**: Top-k sparse coding without thresholding allows weak atoms to contribute, effectively creating dense (not sparse) representations.

**Fix Applied**: Added adaptive thresholding in `topk_sparse_coding()`:
```python
# Only keep atoms with correlation > 10% of maximum
max_corr = abs_corr.max(dim=0, keepdim=True)[0]
threshold = 0.1 * max_corr
threshold_mask = topk_vals > threshold
selected_corr = selected_corr * threshold_mask  # Zero out weak correlations
```

#### 2. **No Weight Decay**

**Problem**: Optimizer used `Adam` without weight decay, allowing unbounded parameter growth.

**Fix**: Changed to `AdamW` with L2 regularization:
```python
optimizer = torch.optim.AdamW(
    params,
    lr=self.learning_rate,
    betas=(self.beta, 0.999),
    weight_decay=1e-4  # L2 regularization
)
```

#### 3. **No L1 Sparsity Regularization**

**Problem**: No explicit penalty on coefficient magnitude to encourage true sparsity.

**Fix**: Added L1 regularization on sparse coefficients:
```python
sparsity_loss = torch.abs(coefficients).mean()
total_loss = (
    recon_loss +
    10 * bottleneck_loss +
    self.perceptual_weight * perceptual_loss +
    self.sparsity_reg_weight * sparsity_loss  # L1 penalty
    ...
)
```

**Configuration**:
```yaml
# configs/model/laser.yaml
sparsity_reg_weight: 0.01  # L1 regularization weight
```

#### 4. **Backprop-Only Mode Defeats Sparsity**

**Problem**: When `use_backprop_only=True`, the model uses:
```python
coefficients = torch.matmul(self.dictionary.t(), patch_tokens)
```
This is **dense** representation (all atoms), not sparse!

**Fix**: Ensure `use_backprop_only: false` in config to enforce sparsity constraint:
```yaml
# configs/model/laser.yaml
use_backprop_only: false  # Use sparse coding, not dense projection
```

### Monitoring Overfitting

Track these metrics during training:

**Healthy Training**:
- `train/sparsity`: 0.03-0.10 (3-10% of atoms active)
- `val/loss` â‰ˆ `train/loss` (small gap)
- `val/psnr` and `val/ssim` improve over epochs

**Overfitting Indicators**:
- `train/sparsity` > 0.5 (too many atoms used)
- `val/loss` >> `train/loss` (large gap)
- `train/psnr` high but `val/psnr` plateaus or decreases

### Additional Regularization Techniques

If overfitting persists, try:

1. **Increase Commitment Cost**:
```yaml
commitment_cost: 1.0  # Up from 0.5
```

2. **Reduce Dictionary Size**:
```yaml
num_embeddings: 128  # Down from 256
```

3. **Increase Sparsity Constraint**:
```yaml
sparsity_reg_weight: 0.05  # Up from 0.01
```

4. **Add Dropout** (requires code changes):
```python
# In encoder/decoder
self.dropout = nn.Dropout(0.1)
```

5. **Data Augmentation** (recommended):
```python
# In data module
transforms = [
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.1, 0.1, 0.1),
    transforms.RandomCrop(size, padding=4),
    ...
]
```

6. **Early Stopping**:
```python
# In train.py
from lightning.pytorch.callbacks import EarlyStopping
early_stop = EarlyStopping(
    monitor='val/loss',
    patience=10,
    mode='min'
)
trainer = pl.Trainer(callbacks=[early_stop, ...])
```

### Expected Improvements

With regularization fixes applied:

1. **True Sparsity**: Sparsity metric should decrease from ~1.0 to intended level:
   - Target: `sparsity_level / num_embeddings = 8/256 â‰ˆ 0.03` (3%)
   
2. **Better Generalization**: Smaller gap between train/val loss

3. **Reduced Overfitting**: Weight decay + L1 penalty prevent memorization

### Verification After Training

Check if fixes worked:
```python
# Low sparsity (good!)
assert metrics['train/sparsity'] < 0.15

# Small train-val gap (good!)
gap = metrics['train/loss'] / metrics['val/loss']
assert 0.8 < gap < 1.2

# Sparsity loss visible
assert metrics['train/sparsity_loss'] > 0
```

## License

MIT
