# LASER: Learnable Adaptive Structured Embedding Representation

This repository provides two autoencoder baselines for image reconstruction:

- **Vector Quantized VAE (VQ-VAE)** â€” discrete latent codes with a learnable codebook.
- **Dictionary Learning VAE (DL-VAE)** â€” sparse dictionary bottleneck trained with Batch OMP.

Generation utilities have been removed for now so you can focus on training and evaluating reconstructions only.

## Features

- ðŸš€ Choice of bottlenecks: vector quantization or dictionary learning
- âš¡ GPU-friendly implementation with AMP-aware sparse coding
- ðŸ“Š Reconstruction quality metrics: MSE, PSNR, SSIM, optional LPIPS/FID
- ðŸ”§ Modular architecture powered by PyTorch Lightning and Hydra

## Installation

```bash
# Clone the repository
git clone https://github.com/helloimlixin/laser.git
cd laser

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scriptsctivate

# Install dependencies
pip install -r requirements.txt
```

## Project Structure

```
â”œâ”€â”€ configs/                # Hydra configuration files
â”‚   â”œâ”€â”€ checkpoint/         # Checkpoint configurations
â”‚   â”œâ”€â”€ data/               # Dataset configurations
â”‚   â”œâ”€â”€ model/              # Model configurations
â”‚   â”œâ”€â”€ train/              # Training configurations
â”‚   â”œâ”€â”€ wandb/              # W&B logging configurations
â”‚   â””â”€â”€ config.yaml         # Main configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # Data modules
â”‚   â”‚   â”œâ”€â”€ cifar10.py
â”‚   â”‚   â”œâ”€â”€ imagenette2.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bottleneck.py   # VQ and dictionary bottlenecks
â”‚   â”‚   â”œâ”€â”€ decoder.py
â”‚   â”‚   â”œâ”€â”€ dlvae.py
â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”œâ”€â”€ lpips.py
â”‚   â”‚   â””â”€â”€ vqvae.py
â””â”€â”€ train.py                # Main training script
```

## Usage

### Training

```bash
# Train VQ-VAE
python train.py model.type=vqvae data=cifar10

# Train DL-VAE
python train.py model.type=dlvae data=cifar10
```

### Running Tests

```bash
pytest tests/test_dlvae.py -q
```

## Configuration

All configuration is managed through Hydra. Adjust the YAML files under `configs/` or override settings directly from the command line, e.g.

```bash
python train.py model.type=dlvae data=celeba train.max_epochs=50
```

## Bottleneck Visualizations

The `tests/test_bottleneck.py` generates comprehensive visualizations comparing Vector Quantization (VQ) and Dictionary Learning (DL) bottlenecks on CelebA data at 128Ã—128 resolution.

### Reconstruction Comparison

Side-by-side comparison of VQ and DL reconstruction quality (K=16 codebook/atoms, S=4 sparsity):

![Reconstruction Comparison](img/reconstruction_comparison.png)

**Key Observations:**
- **VQ (Vector Quantization)**: Maps each pixel to the nearest RGB color from 16 learned codebook entries. Fast but limited to discrete color matching.
- **DL (Dictionary Learning)**: Represents each pixel as a sparse combination of 4 atoms from 16 options. More expressive representation.
- **Error Maps**: DL shows consistently lower error (darker error maps) across all images, especially in complex regions like faces.
- **Quantitative**: DL achieves **10.4Ã— lower MSE** than VQ while using the same 16-entry codebook, demonstrating the power of sparse combinations.

### Code Interpretability Heatmaps

Spatial visualization of how VQ and DL encode the image structure:

![Code Heatmaps](img/code_heatmaps.png)

**Understanding the Heatmaps:**
- **Column 1 (Original)**: Input CelebA images at 128Ã—128 resolution
- **Column 2 (VQ Code Indices)**: Shows which of the 16 codebook entries is assigned to each pixel. Color represents the discrete code index (0-15).
  - Each pixel uses exactly **1 code** from 16 options
  - Spatial patterns show how VQ segments the image into color regions
  - Viridis colormap: Purple (low indices) â†’ Yellow (high indices)
- **Column 3 (DL Sparse Code Strength)**: Shows the L1 norm (sum of absolute coefficients) at each pixel location.
  - Each pixel uses **4 different atoms** with varying weights
  - Darker regions (purple) = weak total activation, brighter regions (yellow) = strong total activation
  - Reveals which image areas require stronger sparse representations
  - L1 norm provides a stable measure of total "activation energy" per location
  - Normalized to [0, 1] using percentile clipping (1st-99th percentile) for clean visualization

**VQ vs DL Encoding:**
- VQ: Discrete, categorical assignment (one-hot selection)
- DL: Continuous, weighted combination (sparse weighted sum)
- DL's flexibility with L1-normalized coefficients enables better reconstruction with the same codebook size

### Channel-wise Comparison

Pixel-level RGB channel analysis comparing original vs reconstructions:

![Channel Comparison](img/vq_channel_comparison.png)

**Analysis:**
- Shows a horizontal slice through the center of the first image across all three color channels
- **Black line**: Original pixel values
- **Blue dashed**: VQ reconstruction
- **Red dotted**: DL reconstruction
- **Key Insight**: DL tracks the original signal more closely than VQ across all channels, especially for smooth gradients and transitions
- Both VQ and DL handle sharp edges well, but DL excels at subtle color variations

### Usage Statistics

How frequently each codebook entry (VQ) or dictionary atom (DL) is utilized:

**VQ Codebook Usage:**
![VQ Usage](img/vq_codebook_usage.png)

- **All 16 codes are used**, showing k-means initialization creates a representative color palette
- Usage distribution is moderately skewed (dominant colors like skin, hair, background are used more)

**Dictionary Atom Usage:**
![DL Usage](img/dictionary_atom_usage.png)

- **All 16 atoms are used** thanks to diversity bonus in OMP selection
- More balanced distribution than VQ due to sparse combination (each pixel can use multiple atoms)
- Top atoms represent dominant color directions, while less-used atoms capture nuanced variations

### Detailed Performance Analysis (K=16, Sparsity=4, 128Ã—128 resolution)

#### Overall Reconstruction Quality

| Method | Overall MSE | Per-Pixel Color Distance | Quality Gain |
|--------|-------------|-------------------------|--------------|
| **VQ** | 0.00905 | 0.0759 | Baseline |
| **DL** | 0.00087 | 0.0163 | **10.4Ã— better MSE**, **21% of VQ color error** |

**Key Insight**: With the simplified greedy OMP, DL reduces per-pixel color distance to just 21% of VQ's error, demonstrating dramatically better perceptual quality. The greedy algorithm provides excellent reconstruction while being simpler and faster than Cholesky-based OMP.

#### Channel-wise Performance

Breaking down reconstruction quality by RGB channel reveals DL's exceptional improvement:

| Channel | VQ MSE | DL MSE | DL Improvement |
|---------|--------|--------|----------------|
| **Red** | 0.0103 | 0.0003 | **30.8Ã— better** |
| **Green** | 0.0084 | 0.0013 | **6.3Ã— better** |
| **Blue** | 0.0085 | 0.0010 | **8.9Ã— better** |

**Analysis**: 
- DL excels particularly in Red channel (30Ã—+ improvement!)
- All channels show significant improvement (6-31Ã— better)
- Green channel shows most modest but still strong improvement (6.3Ã—)
- Balanced performance across all channels indicates proper sparse coding without artifacts

#### Codebook/Atom Utilization

| Method | Entries Used | Distribution | Notes |
|--------|--------------|--------------|-------|
| **VQ** | 16/16 (100%) | Moderately skewed | Each pixel uses exactly 1 code |
| **DL** | 16/16 (100%) | Balanced | Each pixel uses exactly 4 different atoms |

**Full Utilization**: Both methods use all 16 codebook entries/atoms thanks to k-means initialization (VQ) and greedy selection with no-reselection masking (DL), ensuring the full representation capacity is utilized.

#### Inference Speed & Computational Complexity

**Benchmark Setup**: 128Ã—128 resolution, K=16 atoms, S=4 sparsity

| Batch Size | Pixels | VQ Time | VQ Âµs/pixel | DL Time | DL Âµs/pixel | Slowdown |
|------------|--------|---------|-------------|---------|-------------|----------|
| **1** | 16,384 | 1.2 ms | 0.070 | 9.0 ms | 0.551 | **7.9Ã—** |
| **4** | 65,536 | 1.8 ms | 0.028 | 33.0 ms | 0.503 | **17.9Ã—** |
| **8** | 131,072 | 2.6 ms | 0.020 | 64.5 ms | 0.492 | **24.5Ã—** |
| **16** | 262,144 | 5.0 ms | 0.019 | 128.6 ms | 0.490 | **26.0Ã—** |

**Complexity Analysis**:
- **VQ**: O(K Ã— M Ã— N) where K=codebook size, M=channels, N=num_pixels
  - Perfectly vectorizable across batch and spatial dimensions
  - Excellent batching efficiency: per-pixel time drops 3.7Ã— (70ns â†’ 19ns) as batch grows
  - Single matrix multiplication D^T @ X for all pixels simultaneously
  
- **DL**: O(S Ã— K Ã— M Ã— N) where S=sparsity, K=atoms, M=channels, N=num_pixels  
  - **Sequential within each iteration**: correlation â†’ argmax â†’ update â†’ repeat S times
  - Limited batching benefit: per-pixel time only improves 1.1Ã— (551ns â†’ 490ns)
  - Each of S=4 iterations requires KÃ—M matrix-vector products per pixel
  
**Why DL Scales Worse:**
1. **Sequential dependencies**: Each OMP iteration depends on previous iteration's residual
2. **Less vectorization**: VQ does one big matmul; OMP does 4 sequential smaller operations
3. **Memory access**: OMP repeatedly reads/writes residuals; VQ has cleaner access pattern
4. **Fixed overhead**: OMP initialization costs are amortized less efficiently

**Theoretical vs Measured Slowdown:**
- **Theoretical**: 4Ã— (from S=4 sparsity iterations)
- **Measured**: 7.9-26Ã— depending on batch size
- **Gap explained by**: Sequential iteration overhead, less efficient vectorization, memory access patterns

**Scaling Observations**:
- **VQ scales excellently with batch size**: Per-pixel cost drops 3.7Ã— as vectorization efficiency improves
- **DL scaling is limited by sequential OMP iterations**: Each iteration must complete before next begins
- **Relative slowdown grows with batch size**: From 7.9Ã— (batch=1) to 26Ã— (batch=16)
  - This is expected: VQ's better vectorization means it benefits more from larger batches
  - DL's per-pixel time stays nearly constant (~0.50 Âµs) regardless of batch size
- **Both methods remain practical**: VQ at 19-70 ns/pixel, DL at 490-551 ns/pixel
- **Quality advantage dominates**: Despite 8-26Ã— slowdown, DL achieves **10.4Ã— better MSE**

**Speed vs Quality Tradeoff**:
- At batch=4 (typical training batch): DL is 17.9Ã— slower but achieves **10.4Ã— better MSE**
- For batch processing and offline training, the quality gain far outweighs the speed cost
- Both methods support real-time processing: even at batch=16, DL processes 262K pixels in 129ms

#### Patch-Based Dictionary Learning

Using larger patches dramatically reduces computation by processing fewer tokens, with a tradeoff in reconstruction quality:

| Patch Size | Patches/Image | DL Time | Speedup | DL MSE | VQ MSE | Quality vs VQ |
|------------|---------------|---------|---------|--------|--------|---------------|
| **1Ã—1** (pixel) | 65,536 | 33.6 ms | 1.0Ã— | 0.00817 | 0.00964 | **DL 1.2Ã— better** âœ“ |
| **2Ã—2** | 16,384 | 9.3 ms | **3.6Ã—** | 0.01071 | 0.00964 | VQ 1.1Ã— better â‰ˆ |
| **4Ã—4** | 4,096 | 3.7 ms | **9.1Ã—** | 0.06227 | 0.00964 | VQ 6.5Ã— better âœ— |
| **8Ã—8** | 1,024 | 2.3 ms | **14.8Ã—** | 0.09623 | 0.00964 | VQ 10Ã— better âœ— |

**Key Findings**:
- **Pixel-level DL best quality**: 1.2Ã— better than VQ with full spatial resolution
- **2Ã—2 patches competitive**: Only 1.1Ã— worse than VQ but 3.6Ã— faster than pixel-level DL
- **Larger patches trade quality for speed**: 4Ã—4 and 8Ã—8 achieve 9-15Ã— speedup but sacrifice quality
- **Proper initialization critical**: Dictionary must be initialized with k-means on patches (not pixels!)

**Why Larger Patches Degrade**:
1. Higher-dimensional atoms (e.g., 8Ã—8 RGB patch = 192D) harder to represent with fixed sparsity
2. Each patch is single token â†’ coarser spatial granularity than pixel-level
3. K-means initialization on high-D patch space less effective than on pixels
4. Fixed 4-atom sparsity insufficient for complex patch patterns

**Practical Recommendations**:
- **High quality needed**: Use **1Ã—1 (pixel-level)** for 1.2Ã— better MSE than VQ
- **Speed/quality balance**: Use **2Ã—2 patches** for 3.6Ã— speedup with competitive quality (1.1Ã— worse)
- **Maximum speed**: Use **4Ã—4 patches** for 9Ã— speedup if some quality loss acceptable
- **Avoid 8Ã—8**: Too coarse, loses too much detail (10Ã— worse than VQ)

**Conclusion**: **Patch-based DL offers a flexible speed/quality tradeoff**. For most use cases, **2Ã—2 patches** provide the best balance: 3.6Ã— faster than pixel-level with only marginal quality loss. Pixel-level remains best for maximum quality.

### Key Advantages of Dictionary Learning

- âœ“ **Superior reconstruction**: 10.4Ã— lower MSE with same 16-entry codebook
- âœ“ **Sparse representation**: Each pixel uses only 4/16 atoms (25% sparsity) vs VQ's 1/16 selection
- âœ“ **Perceptual quality**: 21% color distance of VQ (nearly 5Ã— improvement)
- âœ“ **Channel balance**: Outperforms VQ on all RGB channels (6-31Ã— better)
- âœ“ **Full utilization**: All 16 atoms actively contribute to reconstruction
- âœ“ **Interpretable**: L1 norm visualization reveals spatial importance patterns (see heatmaps)
- âœ“ **Gradient-friendly**: Supports end-to-end training with proper backpropagation
- âœ“ **Simpler implementation**: Greedy OMP is cleaner and faster than Cholesky-based approaches

### Tradeoffs

- âœ— **Slower inference**: 7.9-26Ã— slower than VQ depending on batch size (larger batches increase relative slowdown)
- âœ— **Memory overhead**: Must store and compute with full dictionary + sparse coefficients  
- âœ— **Computational complexity**: O(K Ã— S Ã— N) vs VQ's O(K Ã— N) where K=atoms, S=sparsity, N=pixels
- âœ— **Worse batching scaling**: VQ benefits more from large batches due to simpler vectorization

**Bottom Line**: Despite being 8-26Ã— slower (depending on batch size), DL achieves **10.4Ã— better reconstruction quality**. Both methods remain practical for real-time use with sub-microsecond per-pixel processing. For applications where quality matters, DL's superior reconstruction far outweighs the speed cost.

### Codebook Visualization in RGB Space

To understand how VQ and DL learn different representations, we can visualize their codebooks/dictionaries in 2D using dimensionality reduction:

![Codebook Embeddings](img/codebook_embeddings.png)

**Key Observations**:
- **PCA (Linear)**: Both VQ and DL spread their atoms across similar RGB subspaces, with the first two principal components capturing ~85% variance
- **t-SNE (Non-linear)**: Reveals local clustering structure - both methods form distinct color clusters (e.g., skin tones, hair colors, backgrounds)
- **UMAP (Manifold)**: Shows the global topology - atoms are distributed along a smooth manifold representing the continuous RGB color space

**Pairwise Distance Analysis**:

![Codebook Distances](img/codebook_distances.png)

- **VQ**: Mean pairwise distance = 2.49 Â± 0.90 (more uniform spacing)
- **DL**: Mean pairwise distance = 2.48 Â± 0.90 (similar distribution)

Both methods achieve similar codebook diversity, but DL's advantage comes from **sparse combinations** (4 atoms per pixel) rather than just better atom selection.

Run codebook visualization:
```bash
conda activate research
pytest tests/test_codebook_visualization.py::test_visualize_codebook_embeddings -v
```

### Technical Implementation

**Simplified Greedy OMP**:
- ðŸš€ **Greedy atom selection**: Simplified OMP without Cholesky decomposition (faster and cleaner)
- ðŸš€ **Vectorized batch processing**: (N, B) tensor format for efficient parallel sparse coding
- ðŸš€ **Float masking**: Uses multiplicative masking instead of boolean indexing for speed
- ðŸš€ **Projection-based coefficients**: Direct inner product computation (no least squares solve)
- ðŸš€ **No-reselection masking**: Ensures each signal uses exactly S distinct atoms

**Visualization Improvements**:
- ðŸ“Š **L1 norm visualization**: Uses sum of absolute coefficients for stable, interpretable heatmaps
- ðŸ“Š **Percentile normalization**: Robust 1-99th percentile clipping for clean contrast
- ðŸ“Š **Fold/unfold mapping**: Properly maps patch-based coefficients to pixel space for visualization
- ðŸ“Š **RGB space embeddings**: PCA, t-SNE, and UMAP projections reveal codebook structure and diversity

Run visualizations:
```bash
conda activate research
pytest tests/test_bottleneck.py::test_bottleneck_visualizations -v

# Update README images after regenerating visualizations
cp tests/artifacts/bottleneck/*.png img/
cp tests/artifacts/codebook_embeddings/*.png img/
```

## License

MIT
