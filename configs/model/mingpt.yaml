type: mingpt
# VQVAE architecture (used to initialize tokenizer backbone)
in_channels: 3
num_hiddens: 128
num_embeddings: 512
embedding_dim: 64
num_residual_blocks: 2
num_residual_hiddens: 64
commitment_cost: 0.25
decay: 0.0
perceptual_weight: 0.0
compute_fid: false

# Pretrained VQVAE checkpoint (leave empty to use randomly initialized VQVAE)
vqvae_ckpt: "/home/xl598/Projects/laser/outputs/checkpoints/run_20251111_134837/vqvae/last.ckpt/checkpoint/mp_rank_00_model_states.pt"
freeze_vqvae: true

# GPT configuration
gpt:
  n_layer: 8
  n_head: 8
  n_embd: 512
  block_size: 0  # 0 => auto infer H_z*W_z at runtime


