# LASER: Learnable Adaptive Structured Embedding Representation

This repository provides two autoencoder baselines for image reconstruction:

- **Vector Quantized VAE (VQ-VAE)** â€” discrete latent codes with a learnable codebook.
- **Dictionary Learning VAE (DL-VAE)** â€” sparse dictionary bottleneck trained with Batch OMP.

Generation utilities have been removed for now so you can focus on training and evaluating reconstructions only.

## Features

- ðŸš€ Choice of bottlenecks: vector quantization or dictionary learning
- âš¡ GPU-friendly implementation with AMP-aware sparse coding
- ðŸ“Š Reconstruction quality metrics: MSE, PSNR, SSIM, optional LPIPS/FID
- ðŸ”§ Modular architecture powered by PyTorch Lightning and Hydra

## Installation

```bash
# Clone the repository
git clone https://github.com/helloimlixin/laser.git
cd laser

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scriptsctivate

# Install dependencies
pip install -r requirements.txt
```

## Project Structure

```
â”œâ”€â”€ configs/                # Hydra configuration files
â”‚   â”œâ”€â”€ checkpoint/         # Checkpoint configurations
â”‚   â”œâ”€â”€ data/               # Dataset configurations
â”‚   â”œâ”€â”€ model/              # Model configurations
â”‚   â”œâ”€â”€ train/              # Training configurations
â”‚   â”œâ”€â”€ wandb/              # W&B logging configurations
â”‚   â””â”€â”€ config.yaml         # Main configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # Data modules
â”‚   â”‚   â”œâ”€â”€ cifar10.py
â”‚   â”‚   â”œâ”€â”€ imagenette2.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bottleneck.py   # VQ and dictionary bottlenecks
â”‚   â”‚   â”œâ”€â”€ decoder.py
â”‚   â”‚   â”œâ”€â”€ dlvae.py
â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”œâ”€â”€ lpips.py
â”‚   â”‚   â””â”€â”€ vqvae.py
â””â”€â”€ train.py                # Main training script
```

## Usage

### Training

```bash
# Train VQ-VAE
python train.py model.type=vqvae data=cifar10

# Train DL-VAE
python train.py model.type=dlvae data=cifar10
```

### Running Tests

```bash
pytest tests/test_dlvae.py -q
```

## Configuration

All configuration is managed through Hydra. Adjust the YAML files under `configs/` or override settings directly from the command line, e.g.

```bash
python train.py model.type=dlvae data=celeba train.max_epochs=50
```

## Bottleneck Visualizations

The `tests/test_bottleneck.py` generates comprehensive visualizations comparing VQ and DL bottlenecks on CelebA data:

### Reconstruction Comparison
VQ vs Dictionary Learning reconstruction quality (K=16 codebook/atoms, S=4 sparsity):

![Reconstruction Comparison](img/bottleneck/reconstruction_comparison.png)

### Code Interpretability Heatmaps
Spatial visualization of VQ indices and DL sparse coefficients:

![Code Heatmaps](img/bottleneck/code_heatmaps.png)

### Channel-wise Comparison
RGB channel-level reconstruction fidelity:

![Channel Comparison](img/bottleneck/vq_channel_comparison.png)

### Usage Statistics

**VQ Codebook Usage:**
![VQ Usage](img/bottleneck/vq_codebook_usage.png)

**Dictionary Atom Usage:**
![DL Usage](img/bottleneck/dictionary_atom_usage.png)

### Performance Summary (K=16, Sparsity=4)

- **VQ**: 2.4 ms inference, MSE=0.00867
- **DL**: 11.8 ms inference (4.9x slower), MSE=0.00258 (3.4x better quality)
- **Atom utilization**: 16/16 atoms used (100% with diversity bonus)

Run visualizations:
```bash
conda activate research
pytest tests/test_bottleneck.py::test_bottleneck_visualizations -v

# Update README images after regenerating visualizations
cp tests/artifacts/bottleneck/*.png img/bottleneck/
```

## License

MIT
